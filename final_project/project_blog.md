# Project Blog Post
Jerry Xu and Liang Qiu
## Project Creation

Among all the definition of Digital Humanity provided on the [what's humanity website](https://whatisdigitalhumanities.com/), this one is the closest to what we would've define it ourself. It quotes "The use of information technology and software with humanities source materials to pursue research questions" by Elizabeth McAulay. 

To us, digital humanity utilizes technology to explore human culture. We set our goal to explore the social media platform because it's a place where huge amount of human interaction is happening every minute. Of all the social media platforms, we chose to do Twitch, the streaming platform. Twitch is an interesting example of how the communities are built around individual streamer and also how different culture exist among those groups. We also get some inspiration from the article that we came across. It's an article about how Twitch is used by people to cope with their difficulties in life. "It is clear from our research that streamers, supported by platforms such as Twitch, do serve an important purpose for people that are active on the platform and are going through a difficult time, even without consciously attempting to do so" ((de Wit et al. Live streams on twitch help viewers cope with difficult periods in life)). "Why is Twitch so popular and how is used to build human subculture" is the ultimate question we are trying to answer by this project. We did came up our research questions around this topic. 

For the tools and methodology used in the project, we decided to use python and some sort of visualization to explore the dataset. We did this not only because python is used as the primary programming language in the course, but also is ranked as the top tools used in humanity research (Barbot et al. WHICH DH TOOLS ARE ACTUALLY USED IN RESEARCH?). We decided to use some sort of statistics and techniques for grouping up data to reflect the changes and pattern of our data.
## Data Collection and Cleaning
From the in class reading "Getting know your data", we learned that we needed to ask questions about available datasets (Krause Data Biographies: Getting to know your data). Where did it come from? Who collected it? How was it collected? Most importantly, why was it collected? Those four questions completes the data biography and by knowing our data, we can discover bias.

We can apply the four questions to our first dataset on kaggle. Kaggle is a really popular site for accessing publicly available datasets. Despite its popularity, we can't not evaluate the credibility of the website as individual publisher doesn't need to go through verification process and the datasets can be purely misleading or fabricated information. That's why we will be asking the four questions. The first dataset that we collected was pretty organized. The author of the dataset is a Twitch enthusiast, and he provides clear motivation for collecting and publishing the dataset. However, he doesn't provide any detail about the collection and curation process of the dataset. As we looked through the content of the dataset, we saw that the values in each column were reasonable. They were not just randomly selected, and they seemed right as we could verify using our own knowledge about Twitch. The dataset has 1000 rows, and no empty value. After evaluation, we came to a conclusion that this dataset is credible and can be used in our project.

We don't need to worry about the data biography for our second dataset because it's collected by ourselves, but we do need to worry about what is being collected. After collecting the data, we found out that the Twitch API interface is pretty limited with what data can be collected. It can only collect the top streamer at the time of collection, and is limited to 100 people. The data returned only has couple useful feature that can be used in our project. Since multiple json dataset are being collected for each day, we spent most of the time putting all the small dataset together, updating and reorganizing the final dataset. We found the process extremely tedious and tiring. We weren't equipped with the skills to curate data automatically, so we had to do it manually. We did go through every row and cross examine every datasets, but we could still potentially make mistakes during the process.
## Analysis
From the class reading “Isn’t it obvious”, Lincoln Mullen speaks on the values of data visualization. He states, “The sense that a visualization is immediately explainable is the result, I think, of the ability of visualizations to rapidly and persuasively communicate large amounts of information.”. With this in mind, we wanted to develop visualizations that can help communicate the data gathered on both datasets. This relates similarly to the quote, “a picture is worth a thousand words” as people are able to view data visualizations and immediately understand the results found in them. In the links below, we developed several data visualizations that allowed us to answer some of our research questions. 

With our datasets not quite having the same data as one another, we have different approaches when it came to analyzing them. We would look at each dataset individually and analyze them accordingly to what data is being offered. To do so, we used Jupiter notebooks as our method of analysis. For the first dataset (the one we obtained off Kaggle), we had two goals in mind; what is the language distribution among the top 1000 streamers, and what is the relationship between language and other statistics (Ex. Mature Content, Peak Viewers, Average Viewers). Our analysis for the first dataset can be found here; https://github.com/jiyanx23/is310-final-project/blob/main/final_project/IS310_twitch_dataset_analysis.ipynb. As for our second dataset (the one we gather off twitch’s API), after updating and reorganizing the data, we wanted to do an analysis on the relationship of mature content and viewership, relationship of language and mature content, genre distribution, and streaming time periods. Our analysis of the second dataset can be found here; https://github.com/jiyanx23/is310-final-project/blob/main/final_project/data_analysis.ipynb. One similarity between both datasets was that they both contain data on the language and mature content. With this in mind, we decided to this as a connection point between each dataset and use this as a way to analyze the similarity and difference between both. 

## Future
From the class reading “Humanities Data: A Necessary Contradiction”, Miriam goes on to talk about the difference between humanists and data scientists. How data scientists view issues through patterns and anomalies found within data to understand why certain things happen. While humanists view issues through a critical thinking viewpoint where they understand the issue from how they view society rather than using data. Miriam later on states, “I think that’s why digital humanities is so challenging and fun, because you’re always holding in your head this tension between the power of computation and the inadequacy of data to truly represent reality.”. Looking at our project, we can understand why Miriam would say that digital humanities is challenging and fun. What was challenging during this project was viewing our topic from both the viewpoints of a data scientist and humanist. We found it difficult to view the topic from a computation aspect while also factoring in a humanities aspect. There were times where we focus heavily on one rather than the other. The challenge to combine both aspects into our analysis and overall, the project was quite a learning curve and a satisfying journey for us. 

For the future, if we were to ever revisit this project, there are several things that we would do differently. The first being to restructure how we collected our data. After we had completed our analysis on the datasets, we found that our data collection was flawed due with the reason being that we haven’t collect enough data (referring to the API dataset). With only having collected data for 4 days, that could had been anomalies that could had happened. For example, major live stream events could had been happening which boosts viewership, streamers doing subathons (where streamers would continue to stream as long as they obtain subs from their viewers) or even taking breaks/vacation.  Another thing we would change/rework on would be our research questions. When we first started on this project, from a humanities point of view, we wanted to explore gender on Twitch. This however didn’t go as plan due to there not being any data on gender to be found on twitch either from outside sources or their API. This led to us having to switch up our questions which resulted in us having numerous questions to explore rather than having a specific one to explore. In the future, we want to look at twitch from a more humanities aspect where we focus more on individual streamers rather having to view Twitch as a whole. 

## Reference
de Wit, Jan, et al. “Live Streams on Twitch Help Viewers Cope with Difficult Periods in Life.” Frontiers, Frontiers, 1 Jan. 1AD, https://www.frontiersin.org/articles/10.3389/fpsyg.2020.586975/full. 

Barbot, Laure, et al. WHICH DH TOOLS ARE ACTUALLY USED IN RESEARCH?, 6 Dec. 2019, https://weltliteratur.net/dh-tools-used-in-research/. 

Krause, Heather. “Data Biographies: Getting to Know Your Data.” Global Investigative Journalism Network, 8 Mar. 2021, https://gijn.org/2017/03/27/data-biographies-getting-to-know-your-data/. 

Mullen, Lincoln. “Isn't It Obvious?: Lincoln Mullen.” Isn't It Obvious? | Lincoln Mullen, 10 Jan. 2018, https://lincolnmullen.com/blog/isnt-it-obvious/. 

Miriam. “Humanities Data: A Necessary Contradiction.” Miriam Posners Blog, 25 June 2015, https://miriamposner.com/blog/humanities-data-a-necessary-contradiction/. 
